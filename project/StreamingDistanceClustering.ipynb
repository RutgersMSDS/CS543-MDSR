{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.util import rddToFileName, TransformFunction\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ilab2.cs.rutgers.edu:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe98f984100>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '52g')\n",
    "SparkContext.setSystemProperty('spark.app.name', 'stars')\n",
    "ssc = StreamingContext(sc, 1) \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gt = nx.Graph()\n",
    "directed_Gt = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 10000\n",
    "\n",
    "#batch_file_folder =  \"/common/home/sdb202/project/temp/\"\n",
    "batch_file_folder =  \"/common/home/milky-way/temp2/\"\n",
    "output_file_folder = \"/common/home/milky-way/temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_for_vector(line):\n",
    "    vector = [float(x) for x in line.split(',')]\n",
    "    return vector\n",
    "\n",
    "def node_filter(n) :\n",
    "    \n",
    "    if n[0][0] >= n[1][0] : # filter out redundant pairs\n",
    "        return False\n",
    "    \n",
    "    ra_1 = n[0][2]\n",
    "    ra_2 = n[1][2]\n",
    "    \n",
    "    d_1 = n[0][3]\n",
    "    d_2 = n[1][3]\n",
    "\n",
    "    # ğ›¾â‰ˆ sqrt([(ğ›¼ğ‘âˆ’ğ›¼ğ‘)cos((ğ›¿ğ‘ + ğ›¿ğ‘) / 2)]2+(ğ›¿ğ‘âˆ’ğ›¿ğ‘)2)\n",
    "    ra_diff = (ra_1 - ra_2)\n",
    "    d_diff = (d_1 - d_2)\n",
    "    d_avg = (d_1 + d_2) / 2\n",
    "    \n",
    "    distance = math.sqrt( ((ra_diff * math.cos(d_avg)) ** 2) + (d_diff ** 2) )\n",
    "    if distance < 0.0001: # filter with distance\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    else :\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \n",
    "def add_edges(n):\n",
    "    Gt.add_edge(n[0][0], n[1][0]) # adding edge from A to B\n",
    "    print(\"--- Adding edge: (\", n[0][0], n[1][0], \") ---\")\n",
    "    return (n[0][0], 1) # return A, 1 : meaning, the A has a neighbor\n",
    "\n",
    "\n",
    "def takeAndPrint(time, rdd):\n",
    "    print(\"--\" + str(time) + \"--\\n\")\n",
    "    try:\n",
    "        taken = rdd.collect()\n",
    "\n",
    "        for record in taken:\n",
    "            with open(output_file_folder + str(time) + \".txt\", \"a\") as myfile:\n",
    "                myfile.write(str(record) + \"\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Got exception: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "star_tile_batches = ssc.textFileStream(batch_file_folder)\\\n",
    "                        .mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)\\\n",
    "                        .map(map_for_vector)\n",
    "                        #.window(1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_pairs = star_tile_batches.transform(lambda rdd: rdd.cartesian(rdd))\n",
    "filtered_star_pairs = star_pairs.filter(node_filter)\n",
    "filtered_star_pairs.foreachRDD(takeAndPrint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-12 04:58:23--\n",
      "\n",
      "--2020-12-12 04:58:24--\n",
      "\n",
      "--2020-12-12 04:58:25--\n",
      "\n",
      "--2020-12-12 04:58:26--\n",
      "\n",
      "--2020-12-12 04:58:27--\n",
      "\n",
      "--2020-12-12 04:58:28--\n",
      "\n",
      "--2020-12-12 04:58:29--\n",
      "\n",
      "--2020-12-12 04:58:30--\n",
      "\n",
      "--2020-12-12 04:58:31--\n",
      "\n",
      "--2020-12-12 04:58:32--\n",
      "\n",
      "--2020-12-12 04:58:33--\n",
      "\n",
      "--2020-12-12 04:58:34--\n",
      "\n",
      "--2020-12-12 04:58:35--\n",
      "\n",
      "--2020-12-12 04:58:36--\n",
      "\n",
      "--2020-12-12 04:58:37--\n",
      "\n",
      "--2020-12-12 04:58:38--\n",
      "\n",
      "--2020-12-12 04:58:39--\n",
      "\n",
      "--2020-12-12 04:58:40--\n",
      "\n",
      "--2020-12-12 04:58:41--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.awaitTerminationOrTimeout(80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYSPARK_PYTHON'] = '/koko/system/anaconda/envs/python38/bin/python3'\n",
    "# print(os.environ['PYSPARK_PYTHON'] )\n",
    "\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/koko/system/anaconda/envs/python38/bin/python3'\n",
    "# print(os.environ['PYSPARK_DRIVER_PYTHON'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_tile_batches.pprint(5)\n",
    "# star_tile_batches.saveAsTextFile(output_file_folder + \"stars\")\n",
    "# filtered_star_pairs.saveAsTextFiles(output_file_folder + \"edges\")\n",
    "\n",
    "# def saveAsTextFile(t, rdd):\n",
    "#     path = rddToFileName(output_file_folder + \"edges\", None, t)\n",
    "#     try:\n",
    "#         rdd.saveAsTextFile(path)\n",
    "#     except Exception as e:\n",
    "#         # after recovered from checkpointing, the foreachRDD may\n",
    "#         # be called twice\n",
    "#         print(str(e))\n",
    "\n",
    "# filtered_star_pairs.foreachRDD(saveAsTextFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# degree_rdd = filtered_star_pairs.map(add_edges) \\\n",
    "#                                 .reduceByKey(lambda val1, val2: val1 + val2) #count degree of each node        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "# stars_df = sqlContext.createDataFrame(star_tile_batches, [\"SOURCEID\", \"RA2000\", \"DEC2000\", \"L\", \"B\", \"J\", \"K\"])\n",
    "# edges_df = sqlContext.createDataFrame(filtered_star_pairs, [\"node1\", \"node2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nx.write_gpickle(Gt, \"test.gpickle\")\n",
    "# Gt = nx.read_gpickle(\"test.gpickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructType\n",
    "# dschema = StructType()\\\n",
    "#                 .add(\"row_id\", \"double\")\\\n",
    "#                 .add(\"source_id\", \"double\")\\\n",
    "#                 .add(\"ra\", \"double\")\\\n",
    "#                 .add(\"d\", \"double\")\\\n",
    "#                 .add(\"l\", \"double\")\\\n",
    "#                 .add(\"b\", \"double\")\\\n",
    "#                 .add(\"j\", \"double\")\\\n",
    "#                 .add(\"k\", \"double\")\\\n",
    "\n",
    "# dfCSV = spark.readStream.option(\"sep\", \",\").option(\"header\", \"false\").schema(dschema).csv(batch_file_folder)\n",
    "# dfCSV.createOrReplaceTempView(\"stars\")\n",
    "# totalSalary = spark.sql(\"select ra,sum(d) from stars group by ra\")\n",
    "# query = totalSalary.writeStream.outputMode(\"complete\")\\\n",
    "#     .option(\"checkpointLocation\", \"hello\")\\\n",
    "#     .format(\"memory\")\\\n",
    "#     .queryName('kothi')\\\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ra_1, d_1, ra_2, d_2 = ( 270.50987177, -29.867119148333302, 270.500969023333, -29.86276306 )\n",
    "\n",
    "ra_diff = (ra_1 - ra_2)\n",
    "d_diff = (d_1 - d_2)\n",
    "d_avg = (d_1 + d_2) / 2\n",
    "distance = math.sqrt( ((ra_diff * math.cos(d_avg)) ** 2) + (d_diff ** 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0043596569087387205"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 3 in Python 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
