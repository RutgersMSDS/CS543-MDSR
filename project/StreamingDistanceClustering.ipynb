{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.util import rddToFileName, TransformFunction\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.clustering import StreamingKMeans\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ilab2.cs.rutgers.edu:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8f9c04c100>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '52g')\n",
    "SparkContext.setSystemProperty('spark.app.name', 'stars')\n",
    "ssc = StreamingContext(sc, 1) \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gt = nx.Graph()\n",
    "directed_Gt = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 10000\n",
    "\n",
    "#batch_file_folder =  \"/common/home/sdb202/project/temp/\"\n",
    "batch_file_folder =  \"/common/home/milky-way/temp2/\"\n",
    "output_file_folder = \"/common/home/milky-way/temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_for_vector(line):\n",
    "    vector = [float(x) for x in line.split(',')]\n",
    "    return vector\n",
    "\n",
    "def node_filter(n) :\n",
    "    \n",
    "    if n[0][0] >= n[1][0] : # filter out redundant pairs\n",
    "        return False\n",
    "    \n",
    "    ra_1 = n[0][2]\n",
    "    ra_2 = n[1][2]\n",
    "    \n",
    "    d_1 = n[0][3]\n",
    "    d_2 = n[1][3]\n",
    "\n",
    "    # ùõæ‚âà sqrt([(ùõºùëé‚àíùõºùëè)cos((ùõøùëé + ùõøùëè) / 2)]2+(ùõøùëé‚àíùõøùëè)2)\n",
    "    ra_diff = (ra_1 - ra_2)\n",
    "    d_diff = (d_1 - d_2)\n",
    "    d_avg = (d_1 + d_2) / 2\n",
    "    \n",
    "    distance = math.sqrt( ((ra_diff * math.cos(d_avg)) ** 2) + (d_diff ** 2) )\n",
    "    if distance < 0.0001: # filter with distance\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    else :\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \n",
    "def add_edges(n):\n",
    "    Gt.add_edge(n[0][0], n[1][0]) # adding edge from A to B\n",
    "    print(\"--- Adding edge: (\", n[0][0], n[1][0], \") ---\")\n",
    "    return (n[0][0], 1) # return A, 1 : meaning, the A has a neighbor\n",
    "\n",
    "\n",
    "def takeAndPrint(time, rdd):\n",
    "    print(\"--\" + str(time) + \"--\\n\")\n",
    "    try:\n",
    "        taken = rdd.collect()\n",
    "\n",
    "        for record in taken:\n",
    "            with open(output_file_folder + str(time) + \".txt\", \"a\") as myfile:\n",
    "                myfile.write(str(record) + \"\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Got exception: \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "star_tile_batches = ssc.textFileStream(batch_file_folder)\\\n",
    "                        .mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if idx == 0 else it)\\\n",
    "                        .map(map_for_vector)\n",
    "                        #.window(1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_pairs = star_tile_batches.transform(lambda rdd: rdd.cartesian(rdd))\n",
    "filtered_star_pairs = star_pairs.filter(node_filter)\n",
    "filtered_star_pairs.foreachRDD(takeAndPrint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-13 05:06:53--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-13 05:12:03--\n",
      "\n",
      "Got exception: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 288.0 failed 1 times, most recent failure: Lost task 3.0 in stage 288.0 (TID 1077, ilab2.cs.rutgers.edu, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "--2020-12-13 05:12:04--\n",
      "\n",
      "--2020-12-13 05:12:05--\n",
      "\n",
      "Got exception: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 290.0 failed 1 times, most recent failure: Lost task 3.0 in stage 290.0 (TID 1082, ilab2.cs.rutgers.edu, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "--2020-12-13 05:12:06--\n",
      "\n",
      "--2020-12-13 05:12:07--\n",
      "\n",
      "Got exception: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 292.0 failed 1 times, most recent failure: Lost task 3.0 in stage 292.0 (TID 1087, ilab2.cs.rutgers.edu, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "--2020-12-13 05:12:08--\n",
      "\n",
      "--2020-12-13 05:12:09--\n",
      "\n",
      "Got exception: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 294.0 failed 1 times, most recent failure: Lost task 2.0 in stage 294.0 (TID 1091, ilab2.cs.rutgers.edu, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "--2020-12-13 05:12:10--\n",
      "\n",
      "--2020-12-13 05:12:11--\n",
      "\n",
      "Got exception: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 296.0 failed 1 times, most recent failure: Lost task 2.0 in stage 296.0 (TID 1096, ilab2.cs.rutgers.edu, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "--2020-12-13 05:12:12--\n",
      "\n",
      "--2020-12-13 05:12:13--\n",
      "\n",
      "Got exception: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 298.0 failed 1 times, most recent failure: Lost task 2.0 in stage 298.0 (TID 1101, ilab2.cs.rutgers.edu, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "--2020-12-13 05:12:14--\n",
      "\n",
      "--2020-12-13 05:12:15--\n",
      "\n",
      "Got exception: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 300.0 failed 1 times, most recent failure: Lost task 3.0 in stage 300.0 (TID 1107, ilab2.cs.rutgers.edu, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1972)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1971)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1971)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:950)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:950)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2203)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:752)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2093)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2133)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n",
      "    process()\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/koko/system/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 271, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in map_for_vector\n",
      "  File \"<ipython-input-5-4ea1ac71de53>\", line 2, in <listcomp>\n",
      "ValueError: could not convert string to float: ''\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:295)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:607)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:383)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:218)\n",
      "\n",
      "--2020-12-13 05:12:16--\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.awaitTerminationOrTimeout(80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['PYSPARK_PYTHON'] = '/koko/system/anaconda/envs/python38/bin/python3'\n",
    "# print(os.environ['PYSPARK_PYTHON'] )\n",
    "\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/koko/system/anaconda/envs/python38/bin/python3'\n",
    "# print(os.environ['PYSPARK_DRIVER_PYTHON'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_tile_batches.pprint(5)\n",
    "# star_tile_batches.saveAsTextFile(output_file_folder + \"stars\")\n",
    "# filtered_star_pairs.saveAsTextFiles(output_file_folder + \"edges\")\n",
    "\n",
    "# def saveAsTextFile(t, rdd):\n",
    "#     path = rddToFileName(output_file_folder + \"edges\", None, t)\n",
    "#     try:\n",
    "#         rdd.saveAsTextFile(path)\n",
    "#     except Exception as e:\n",
    "#         # after recovered from checkpointing, the foreachRDD may\n",
    "#         # be called twice\n",
    "#         print(str(e))\n",
    "\n",
    "# filtered_star_pairs.foreachRDD(saveAsTextFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# degree_rdd = filtered_star_pairs.map(add_edges) \\\n",
    "#                                 .reduceByKey(lambda val1, val2: val1 + val2) #count degree of each node        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "# stars_df = sqlContext.createDataFrame(star_tile_batches, [\"SOURCEID\", \"RA2000\", \"DEC2000\", \"L\", \"B\", \"J\", \"K\"])\n",
    "# edges_df = sqlContext.createDataFrame(filtered_star_pairs, [\"node1\", \"node2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nx.write_gpickle(Gt, \"test.gpickle\")\n",
    "# Gt = nx.read_gpickle(\"test.gpickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructType\n",
    "# dschema = StructType()\\\n",
    "#                 .add(\"row_id\", \"double\")\\\n",
    "#                 .add(\"source_id\", \"double\")\\\n",
    "#                 .add(\"ra\", \"double\")\\\n",
    "#                 .add(\"d\", \"double\")\\\n",
    "#                 .add(\"l\", \"double\")\\\n",
    "#                 .add(\"b\", \"double\")\\\n",
    "#                 .add(\"j\", \"double\")\\\n",
    "#                 .add(\"k\", \"double\")\\\n",
    "\n",
    "# dfCSV = spark.readStream.option(\"sep\", \",\").option(\"header\", \"false\").schema(dschema).csv(batch_file_folder)\n",
    "# dfCSV.createOrReplaceTempView(\"stars\")\n",
    "# totalSalary = spark.sql(\"select ra,sum(d) from stars group by ra\")\n",
    "# query = totalSalary.writeStream.outputMode(\"complete\")\\\n",
    "#     .option(\"checkpointLocation\", \"hello\")\\\n",
    "#     .format(\"memory\")\\\n",
    "#     .queryName('kothi')\\\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "ra_1, d_1, ra_2, d_2 = ( 270.50987177, -29.867119148333302, 270.500969023333, -29.86276306 )\n",
    "\n",
    "ra_diff = (ra_1 - ra_2)\n",
    "d_diff = (d_1 - d_2)\n",
    "d_avg = (d_1 + d_2) / 2\n",
    "distance = math.sqrt( ((ra_diff * math.cos(d_avg)) ** 2) + (d_diff ** 2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0043596569087387205"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 3 in Python 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
